{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSlKiwVfSw90"
      },
      "source": [
        "## 1. Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul6kqY53Qrs9"
      },
      "source": [
        "### 1.1 Define the State and Action Space\n",
        "\n",
        "Define the state space, which is a $6\\times 6$ grid world.\n",
        "\n",
        "In addition,\n",
        "\n",
        "- the start state is at $(0,0)$ -- the upper left corner\n",
        "- the goal state is at $(5,5)$ -- the lower right corner\n",
        "\n",
        "In addition, there are some obstacles.\n",
        "\n",
        "The actions are defined based on the structure of the grid world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhSB2E1k5Hf1",
        "outputId": "9415490b-6743-4465-9fde-2da0a0300ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S . . . . . \n",
            ". X X . . . \n",
            ". . X . . . \n",
            ". . X . . . \n",
            ". . . . . . \n",
            ". . . . . G \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the grid world environment\n",
        "grid_size = 6\n",
        "obstacles = [(1, 1), (1, 2), (2, 2), (3,2)]  # Define obstacles\n",
        "start_state = (0, 0)\n",
        "goal_state = (5, 5)\n",
        "\n",
        "# Define actions\n",
        "action_set = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
        "\n",
        "# Plot the grid world\n",
        "def plot_grid():\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == start_state:\n",
        "                print(\"S\", end=\" \")\n",
        "            elif (i, j) == goal_state:\n",
        "                print(\"G\", end=\" \")\n",
        "            elif (i, j) in obstacles:\n",
        "                print(\"X\", end=\" \")\n",
        "            else:\n",
        "                print(\".\", end=\" \")\n",
        "        print()\n",
        "\n",
        "plot_grid()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfotiWfuTv5R"
      },
      "source": [
        "Some additional functions about the environment, for example,\n",
        "\n",
        "- evaluate whether a state is valid\n",
        "- get the next step based on the current state and action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NmFyxX4_TzfP"
      },
      "outputs": [],
      "source": [
        "def is_valid_state(state):\n",
        "    x, y = state\n",
        "    return (0 <= x < grid_size) and (0 <= y < grid_size) and (state not in obstacles)\n",
        "\n",
        "# Modification for (a)\n",
        "def get_next_state_stochastic(current_state, action):\n",
        "\n",
        "    # Modification for (a)\n",
        "    p = random.random()\n",
        "    if p < 0.85:\n",
        "      chosen = action\n",
        "    elif p < 0.90:\n",
        "      chosen = (action[1], -action[0])\n",
        "    else:\n",
        "      chosen = (-action[1], action[0])\n",
        "    next_state = (current_state[0] + chosen[0], current_state[1] + chosen[1])\n",
        "\n",
        "    if is_valid_state(next_state):\n",
        "        # If the next state is valid\n",
        "        return next_state\n",
        "    else:\n",
        "        # Otherwise, stay in the same place\n",
        "        return current_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQy7OQslS0S3"
      },
      "source": [
        "### 1.2 Reward Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGR-tidRTDSW"
      },
      "source": [
        "The reward function differentiates three conditions\n",
        "\n",
        "- Achieve the goal\n",
        "- Hit an obstacle\n",
        "- Move one step: not that if this is a negative value, it means every step the agent takes, it will get a negative reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UdKIQEnLSvDs"
      },
      "outputs": [],
      "source": [
        "def get_reward(state):\n",
        "    if state == goal_state:\n",
        "        return 10\n",
        "    elif state in obstacles:\n",
        "        return -5\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-rMokVDTgDh"
      },
      "source": [
        "### 1.3 Define the $Q(s,a)$ Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrpGCLI_Qzhi"
      },
      "source": [
        "Define the Q function $Q(s,a)$. When both the environment and the actions are discrete, we can use a table to represent this $Q$ function.\n",
        "\n",
        "Note that, all cells are initialized as $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mkvVGCGPQIT4"
      },
      "outputs": [],
      "source": [
        "# Initialize Q-table\n",
        "q_table = np.zeros((grid_size, grid_size, 4))  # 4 actions: up, down, left, right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s68sh7rwUGux"
      },
      "source": [
        "### 1.4 $\\epsilon$-greedy Policy\n",
        "\n",
        "Since $Q(s,a)$ is the not the optimal function during learning, a typical way of constructing a policy function is to use an $\\epsilon$-greedy strategy.\n",
        "\n",
        "$$a = \\left\\{\\begin{array}{ll} \\arg\\max_a Q(s,a) & \\text{with probability}~ 1-\\epsilon \\\\ \\text{a random action} & \\epsilon\\\\ \\end{array} \\right.$$\n",
        "\n",
        "The construction explicitly balances the idea of exploration vs. exploitation.\n",
        "\n",
        "Note that, the action list is {up, down, left, right}.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g7jKLBRhVvJL"
      },
      "outputs": [],
      "source": [
        "def choose_action(state, epsilon):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(action_set)  # Explore\n",
        "    else:\n",
        "        return action_set[np.argmax(q_table[state[0], state[1], :])]  # Exploit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTfPMY8YXHhZ"
      },
      "source": [
        "### 1.5 Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqKHmOf69fg"
      },
      "source": [
        "$$Q(s_t, a_t)\\leftarrow Q(s_t, a_t) + \\eta\\cdot (r_t + \\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex7OXCrLXIeb",
        "outputId": "ffa70001-f4fb-44da-e181-5e4f1eeea81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.8\n",
        "discount_factor = 0.95\n",
        "epsilon = 0.1  # Exploration rate\n",
        "num_episodes = 1000\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    current_state = start_state\n",
        "    while current_state != goal_state:\n",
        "        action = choose_action(current_state, epsilon)\n",
        "\n",
        "        # Modification for (b)\n",
        "        next_state = get_next_state_stochastic(current_state, action)\n",
        "\n",
        "        reward = get_reward(next_state)\n",
        "        action_index = action_set.index(action)\n",
        "\n",
        "        q_table[current_state[0], current_state[1], action_index] = q_table[current_state[0], current_state[1], action_index] + learning_rate * (\n",
        "                reward + discount_factor * np.max(q_table[next_state[0], next_state[1], :]) - q_table[current_state[0], current_state[1], action_index]\n",
        "        )\n",
        "        current_state = next_state\n",
        "\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nX1f4tnXzv0"
      },
      "source": [
        "Print out the $Q(s,a)$ function/table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAzL85VOXzPy",
        "outputId": "8d83359b-f670-4fb5-e93a-d037a9553a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The four actions in order are: Right, Left, Down, Up\n",
            "At state (0, 0), [-2.55886739 -2.91836362 -1.29154499 -2.94193794]\n",
            "At state (0, 1), [-0.95870975 -2.80853581 -1.7933103  -2.63842151]\n",
            "At state (0, 2), [ 0.43357907 -2.02591185 -1.70618109 -1.88528106]\n",
            "At state (0, 3), [-0.09397467 -0.94005127  1.8116372  -0.86293304]\n",
            "At state (0, 4), [-2.68346967 -2.40373599  0.76463684 -2.15188991]\n",
            "At state (0, 5), [-2.72132741 -4.30976021  2.50526258 -3.32724556]\n",
            "At state (1, 0), [-2.43506935 -2.35269613 -0.53093328 -3.03487833]\n",
            "At state (1, 1), [0. 0. 0. 0.]\n",
            "At state (1, 2), [0. 0. 0. 0.]\n",
            "At state (1, 3), [ 1.95803909  0.39929958  3.02078726 -0.89488129]\n",
            "At state (1, 4), [ 3.83217962 -1.70417489  1.60918463  0.1393552 ]\n",
            "At state (1, 5), [-1.74481358 -2.43258474  4.52058278  0.89716346]\n",
            "At state (2, 0), [-2.9931751  -2.31522414  0.63601675 -1.4996405 ]\n",
            "At state (2, 1), [-3.13536995 -3.16868888  1.92414162 -3.07845969]\n",
            "At state (2, 2), [0. 0. 0. 0.]\n",
            "At state (2, 3), [2.38662578 2.09964221 2.99714673 1.62786658]\n",
            "At state (2, 4), [3.03498273 1.55212156 3.40691499 1.93421605]\n",
            "At state (2, 5), [4.86495166 3.76773952 5.62870553 2.87435322]\n",
            "At state (3, 0), [ 0.82805264 -2.76924873  1.61809346 -1.23072092]\n",
            "At state (3, 1), [ 0.85020926  0.19542403  1.81968151 -1.61178424]\n",
            "At state (3, 2), [0. 0. 0. 0.]\n",
            "At state (3, 3), [3.35820762 3.30234729 4.93517512 2.14389311]\n",
            "At state (3, 4), [3.48651656 4.10595554 5.72180226 4.32746037]\n",
            "At state (3, 5), [5.01227677 4.98255074 7.79289272 4.38558822]\n",
            "At state (4, 0), [ 2.26944933 -1.51570786  1.72424042  0.30480781]\n",
            "At state (4, 1), [2.2562402  1.14206205 3.95479168 0.89612123]\n",
            "At state (4, 2), [5.03946147 1.19706348 4.09872317 3.87203954]\n",
            "At state (4, 3), [6.94203823 3.7987007  6.49864091 4.11620829]\n",
            "At state (4, 4), [6.26484094 5.08143485 8.14431531 5.6603705 ]\n",
            "At state (4, 5), [6.83340391 7.36700572 9.99755918 5.32846889]\n",
            "At state (5, 0), [ 4.08498123 -2.30528    -0.18972404 -0.39257845]\n",
            "At state (5, 1), [5.58876659 0.13440572 0.59297569 0.75110866]\n",
            "At state (5, 2), [7.057894   3.39993174 4.11750157 4.33147611]\n",
            "At state (5, 3), [8.49999326 4.72046093 6.1117117  5.28583618]\n",
            "At state (5, 4), [9.99999995 6.85008015 6.97076873 6.95089051]\n",
            "At state (5, 5), [0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(\"The four actions in order are: Right, Left, Down, Up\")\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        print(f\"At state ({i}, {j}), {q_table[i,j,:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKRtNnuBXNYW"
      },
      "source": [
        "### 1.6 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufmaej6dXPz8",
        "outputId": "efc5dd60-9d1a-4c98-edb3-00eb303896be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At state (0, 0): [-2.55886739 -2.91836362 -1.29154499 -2.94193794]\n",
            "At state (1, 0): [-2.43506935 -2.35269613 -0.53093328 -3.03487833]\n",
            "At state (2, 0): [-2.9931751  -2.31522414  0.63601675 -1.4996405 ]\n",
            "At state (3, 0): [ 0.82805264 -2.76924873  1.61809346 -1.23072092]\n",
            "At state (4, 0): [ 2.26944933 -1.51570786  1.72424042  0.30480781]\n",
            "At state (4, 1): [2.2562402  1.14206205 3.95479168 0.89612123]\n",
            "At state (5, 1): [5.58876659 0.13440572 0.59297569 0.75110866]\n",
            "At state (5, 2): [7.057894   3.39993174 4.11750157 4.33147611]\n",
            "At state (5, 3): [8.49999326 4.72046093 6.1117117  5.28583618]\n",
            "At state (5, 4): [9.99999995 6.85008015 6.97076873 6.95089051]\n",
            "Optimal Path:\n",
            "\n",
            "S . . . . . \n",
            "- X X . . . \n",
            "- . X . . . \n",
            "- . X . . . \n",
            "- - . . . . \n",
            ". - - - - G \n"
          ]
        }
      ],
      "source": [
        "# Example usage: Find the best path from start to goal\n",
        "current_state = start_state\n",
        "path = [current_state]\n",
        "while current_state != goal_state:\n",
        "    q_values = q_table[current_state[0], current_state[1], :]\n",
        "    print(f\"At state {current_state}: {q_values}\")\n",
        "    action = action_set[np.argmax(q_values)]\n",
        "\n",
        "    # Modification for (b)\n",
        "    current_state = get_next_state_stochastic(current_state, action)\n",
        "\n",
        "    path.append(current_state)\n",
        "print(\"Optimal Path:\\n\")\n",
        "\n",
        "# Plot the path\n",
        "def plot_grid():\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            if (i, j) == start_state:\n",
        "                print(\"S\", end=\" \")\n",
        "            elif (i, j) == goal_state:\n",
        "                print(\"G\", end=\" \")\n",
        "            elif (i, j) in obstacles:\n",
        "                print(\"X\", end=\" \")\n",
        "            elif (i, j) in path:\n",
        "                print(\"-\", end=\" \")\n",
        "            else:\n",
        "                print(\".\", end=\" \")\n",
        "        print()\n",
        "\n",
        "plot_grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkfyjUyuLPJD"
      },
      "source": [
        "# 2 Policy Gradient\n",
        "\n",
        "To make the gradient computation easier, this code uses PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryutaKyXLPJD"
      },
      "source": [
        "### 2.1 Define Policy Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XkxNsM1_LPJD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 24)\n",
        "        self.fc2 = nn.Linear(24, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "# Initialize policy network\n",
        "\n",
        "# Modification for (c)\n",
        "input_size = 2 + len(action_set)\n",
        "\n",
        "output_size = 4  # up, down, left, right (as before)\n",
        "policy = PolicyNetwork(input_size, output_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrUlYvQQLPJD"
      },
      "source": [
        "### 2.2 Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "C7H_4L_GLPJD"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "num_episodes = 3000 # number of episodes (examples) for training\n",
        "max_steps = 100 # maximum steps per episode, to avoid infinite loops\n",
        "gamma = 0.99 # discount factor\n",
        "optimizer = optim.Adam(policy.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_NUcmTLPJD"
      },
      "source": [
        "### 2.3 Learning with Policy Gradient\n",
        "\n",
        "The implementation of the following code block contains the three steps as explained in class\n",
        "\n",
        "- Step 1: sample from policy function\n",
        "- Step 2: calculate the return\n",
        "- Step 3: calculate the gradient and update the policy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "igms01G1LPJD",
        "outputId": "101f6cf3-5faa-4d0a-b498-500a222a66fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: -83, Steps: 94\n",
            "Episode 500, Reward: -15, Steps: 26\n",
            "Episode 1000, Reward: 0, Steps: 11\n",
            "Episode 1500, Reward: 0, Steps: 11\n",
            "Episode 2000, Reward: -7, Steps: 18\n",
            "Episode 2500, Reward: 1, Steps: 10\n",
            "DONE\n"
          ]
        }
      ],
      "source": [
        "# Storing intermediate results\n",
        "all_rewards = []\n",
        "all_lengths = []\n",
        "\n",
        "# Policy Gradient algorithm\n",
        "for episode in range(num_episodes):\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    current_state = start_state\n",
        "    episode_reward = 0\n",
        "\n",
        "    # Modification for (c)\n",
        "    last_action = None\n",
        "\n",
        "    # ===========================\n",
        "    # Step 1: Sample from the policy function\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        # Modification for (c)\n",
        "        one_hot = [0] * len(action_set)\n",
        "        if last_action is not None:\n",
        "            one_hot[action_set.index(last_action)] = 1\n",
        "        state_tensor = torch.FloatTensor([current_state[0], current_state[1]] + one_hot)\n",
        "\n",
        "        # Get action probabilities\n",
        "        probs = policy(state_tensor)\n",
        "\n",
        "        # Sample action\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action_idx = action_dist.sample()\n",
        "        # print(f\"action_idx: {action_idx}\")\n",
        "        action = action_set[action_idx]\n",
        "\n",
        "        # Take action\n",
        "\n",
        "        # Modification for (c)\n",
        "        next_state = get_next_state_stochastic(current_state, action)\n",
        "\n",
        "        reward = get_reward(next_state)\n",
        "\n",
        "        # Store experience\n",
        "        states.append(state_tensor)\n",
        "        actions.append(action_idx)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        episode_reward += reward\n",
        "        current_state = next_state\n",
        "\n",
        "        if current_state == goal_state:\n",
        "            break\n",
        "\n",
        "    # ===========================\n",
        "    # Step 2: Calculate returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "    returns = torch.FloatTensor(returns)\n",
        "\n",
        "    # Normalize returns\n",
        "    if len(returns) > 1:\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "\n",
        "    # =============================\n",
        "    # Step 3: Calculate the gradient and update the policy\n",
        "    policy_loss = []\n",
        "    for log_prob, G in zip([torch.log(policy(states[i])[actions[i]]) for i in range(len(states))], returns):\n",
        "        policy_loss.append(-log_prob * G)\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    all_rewards.append(episode_reward)\n",
        "    all_lengths.append(step + 1)\n",
        "\n",
        "    if episode % 500 == 0:\n",
        "        print(f\"Episode {episode}, Reward: {episode_reward}, Steps: {step+1}\")\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLHx4cADLPJE"
      },
      "source": [
        "### 2.4 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RgR5qQcgLPJE",
        "outputId": "3eb29585-d402-4005-e26b-785529e3ac4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Gradient Path:\n",
            "S + + + + . \n",
            ". X X . + . \n",
            ". . X . + . \n",
            ". . X . + . \n",
            ". . . . + + \n",
            ". . . . . G \n"
          ]
        }
      ],
      "source": [
        "current_state = start_state\n",
        "pg_path = [current_state]\n",
        "\n",
        "while current_state != goal_state and len(pg_path) < grid_size*grid_size:\n",
        "\n",
        "    # Modification for (c)\n",
        "    one_hot = [0] * len(action_set)\n",
        "    if last_action is not None:\n",
        "        one_hot[action_set.index(last_action)] = 1\n",
        "    state_tensor = torch.FloatTensor([current_state[0], current_state[1]] + one_hot)\n",
        "\n",
        "    probs = policy(state_tensor)\n",
        "    action_idx = probs.argmax().item()\n",
        "    action = action_set[action_idx]\n",
        "\n",
        "    # Modification for (c)\n",
        "    current_state = get_next_state_stochastic(current_state, action)\n",
        "\n",
        "    pg_path.append(current_state)\n",
        "\n",
        "    # Modification for (c)\n",
        "    last_action = action\n",
        "\n",
        "print(\"Policy Gradient Path:\")\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        if (i, j) == start_state:\n",
        "            print(\"S\", end=\" \")\n",
        "        elif (i, j) == goal_state:\n",
        "            print(\"G\", end=\" \")\n",
        "        elif (i, j) in obstacles:\n",
        "            print(\"X\", end=\" \")\n",
        "        elif (i, j) in pg_path:\n",
        "            print(\"+\", end=\" \")\n",
        "        else:\n",
        "            print(\".\", end=\" \")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}